{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Record data\n",
    "* Prepare input layer\n",
    "* Create a layer with the necessary convolutions\n",
    "* create the output layer\n",
    "* create the sequential model of nvidia using the prepared model\n",
    "* Compile the model using the right optimizer and loss calculation function\n",
    "* Split the data into the train set and validation set\n",
    "* Use generator to pre-process the images\n",
    "* train the model with batch size, number of epochs and model fit generator\n",
    "\n",
    "Input for learning is the images and the prediction is the steering angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import csv\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Flatten,Dense,Lambda,Cropping2D,GlobalAveragePooling2D,Convolution2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5,input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "model.add(Convolution2D(24,(5,5),subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(36,(5,5),subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(48,(5,5),subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(64,(3,3),activation='relu'))\n",
    "model.add(Convolution2D(64,(3,3),activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 36\n",
    "epochs = 5\n",
    "\n",
    "samples =[]\n",
    "with open('data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        samples.append(row)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:(offset+batch_size//3)]\n",
    "            images = []\n",
    "            measurements = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                steering_center = float(row[3])\n",
    "                correction = 0.25\n",
    "                steering_left = steering_center + correction\n",
    "                steering_right = steering_center - correction\n",
    "        \n",
    "                for i in range(3):\n",
    "                    source_path = row[i]\n",
    "                    filename = source_path.split('/')[-1]\n",
    "                    current_path = 'data/IMG/'\n",
    "                    if i==0:\n",
    "                        img = np.asarray(mpimg.imread(current_path + filename))\n",
    "                        images.append(img)\n",
    "                        measurements.append(steering_center)\n",
    "                        images.append(np.fliplr(img))\n",
    "                        measurements.append(steering_center*(-1))\n",
    "                    elif i==1:\n",
    "                        img = np.asarray(mpimg.imread(current_path + filename))\n",
    "                        images.append(img)\n",
    "                        measurements.append(steering_left)\n",
    "                        images.append(np.fliplr(img))\n",
    "                        measurements.append(steering_left*(-1))\n",
    "                    elif i==2:\n",
    "                        img = np.asarray(mpimg.imread(current_path + filename))\n",
    "                        images.append(img)\n",
    "                        measurements.append(steering_right)\n",
    "                        images.append(np.fliplr(img))\n",
    "                        measurements.append(steering_left*(-1))\n",
    "                    \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(measurements)\n",
    "                \n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "# Note: we aren't using callbacks here since we only are using 5 epochs to conserve GPU time\n",
    "object_X1 = model.fit_generator(train_generator, steps_per_epoch=np.ceil(len(train_samples)/batch_size),\n",
    "                                epochs=epochs,verbose=1,validation_data=validation_generator,\n",
    "                                validation_steps=np.ceil(len(validation_samples)/batch_size))\n",
    "\n",
    "print(object_X1.history.keys())\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(object_X1.history['loss'])\n",
    "plt.plot(object_X1.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kishan\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "import csv\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Flatten,Dense,Lambda,Cropping2D,GlobalAveragePooling2D,Convolution2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn\n",
    "\n",
    "samples =[]\n",
    "with open('data_hemanth/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        samples.append(row)\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/workspace/CarND-Behavioral-Cloning-P3/Data_hemanth/IMG/center_2020_06_19_19_13_05_286.jpg', '/home/workspace/CarND-Behavioral-Cloning-P3/Data_hemanth/IMG/left_2020_06_19_19_13_05_286.jpg', '/home/workspace/CarND-Behavioral-Cloning-P3/Data_hemanth/IMG/right_2020_06_19_19_13_05_286.jpg', '0', '0', '0', '1.517092E-06']\n"
     ]
    }
   ],
   "source": [
    "print(samples[0])\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=18):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:(offset+batch_size)]\n",
    "            print(offset)\n",
    "            images = []\n",
    "            measurements = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                steering_center = float(batch_sample[3])\n",
    "                correction = 0.10\n",
    "                steering_left = steering_center + correction\n",
    "                steering_right = steering_center - correction\n",
    "                \n",
    "                for i in range(3):\n",
    "                    source_path = batch_sample[0]\n",
    "                    filename = source_path.split('/')[-1]\n",
    "                    current_path = 'Data_hemanth/IMG/'\n",
    "                    if i==0:\n",
    "                        img = np.asarray(mpimg.imread(current_path + filename))\n",
    "                        images.append(img)\n",
    "                        measurements.append(steering_center)\n",
    "                        images.append(np.fliplr(img))\n",
    "                        measurements.append(steering_center*(-1))\n",
    "                \n",
    "                    elif i==1:\n",
    "                        img = np.asarray(mpimg.imread(current_path + filename))\n",
    "                        images.append(img)\n",
    "                        measurements.append(steering_left)\n",
    "                        images.append(np.fliplr(img))\n",
    "                        measurements.append(steering_left*(-1))\n",
    "                    elif i==2:\n",
    "                        img = np.asarray(mpimg.imread(current_path + filename))\n",
    "                        images.append(img)\n",
    "                        measurements.append(steering_right)\n",
    "                        images.append(np.fliplr(img))\n",
    "                        measurements.append(steering_right*(-1))\n",
    "                \n",
    "                    \n",
    "                X_train = np.array(images)\n",
    "                y_train = np.array(measurements)\n",
    "                \n",
    "                yield shuffle(X_train, y_train)\n",
    "batch_size=18\n",
    "\n",
    "def train_generator():\n",
    "    while(True):\n",
    "        yield generator(train_samples, batch_size=batch_size)\n",
    "\n",
    "def validation_generator():\n",
    "    while(True):\n",
    "        yield generator(validation_samples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1325e1b939a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "X_train, y_train = next(train_generator())\n",
    "print(y_train[0])\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5,input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "model.add(Convolution2D(24,(5,5),subsample=(2,2),activation='elu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Convolution2D(36,(5,5),subsample=(2,2),activation='elu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Convolution2D(48,(5,5),subsample=(2,2),activation='elu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Convolution2D(64,(3,3),activation='elu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Convolution2D(64,(3,3),activation='elu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=18, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, ELU, Flatten, Dropout, Dense, Lambda, MaxPooling2D\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import cv2\n",
    "\n",
    "rows, cols, ch = 64, 64, 3\n",
    "\n",
    "TARGET_SIZE = (64, 64)\n",
    "\n",
    "\n",
    "def augment_brightness_camera_images(image):\n",
    "    '''\n",
    "    :param image: Input image\n",
    "    :return: output image with reduced brightness\n",
    "    '''\n",
    "\n",
    "    # convert to HSV so that its easy to adjust brightness\n",
    "    image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # randomly generate the brightness reduction factor\n",
    "    # Add a constant so that it prevents the image from being completely dark\n",
    "    random_bright = .25+np.random.uniform()\n",
    "\n",
    "    # Apply the brightness reduction to the V channel\n",
    "    image1[:,:,2] = image1[:,:,2]*random_bright\n",
    "\n",
    "    # convert to RBG again\n",
    "    image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)\n",
    "    return image1\n",
    "\n",
    "\n",
    "def resize_to_target_size(image):\n",
    "    return cv2.resize(image, TARGET_SIZE)\n",
    "\n",
    "\n",
    "def crop_and_resize(image):\n",
    "    '''\n",
    "    :param image: The input image of dimensions 160x320x3\n",
    "    :return: Output image of size 64x64x3\n",
    "    '''\n",
    "    cropped_image = image[55:135, :, :]\n",
    "    processed_image = resize_to_target_size(cropped_image)\n",
    "    return processed_image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = crop_and_resize(image)\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    #Normalize image\n",
    "    image = image/255.0 - 0.5\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_augmented_row(row):\n",
    "    steering = row['steering']\n",
    "\n",
    "    # randomly choose the camera to take the image from\n",
    "    camera = np.random.choice(['center', 'left', 'right'])\n",
    "\n",
    "    # adjust the steering angle for left anf right cameras\n",
    "    if camera == 'left':\n",
    "        steering += 0.25\n",
    "    elif camera == 'right':\n",
    "        steering -= 0.25\n",
    "\n",
    "    image = load_img(\"udacity_data/\" + row[camera].strip())\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # decide whether to horizontally flip the image:\n",
    "    # This is done to reduce the bias for turning left that is present in the training data\n",
    "    flip_prob = np.random.random()\n",
    "    if flip_prob > 0.5:\n",
    "        # flip the image and reverse the steering angle\n",
    "        steering = -1*steering\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "    # Apply brightness augmentation\n",
    "    image = augment_brightness_camera_images(image)\n",
    "\n",
    "    # Crop, resize and normalize the image\n",
    "    image = preprocess_image(image)\n",
    "    return image, steering\n",
    "\n",
    "\n",
    "def get_data_generator(data_frame, batch_size=32):\n",
    "    N = data_frame.shape[0]\n",
    "    batches_per_epoch = N // batch_size\n",
    "\n",
    "    i = 0\n",
    "    while(True):\n",
    "        start = i*batch_size\n",
    "        end = start+batch_size - 1\n",
    "\n",
    "        X_batch = np.zeros((batch_size, 64, 64, 3), dtype=np.float32)\n",
    "        y_batch = np.zeros((batch_size,), dtype=np.float32)\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        # slice a `batch_size` sized chunk from the dataframe\n",
    "        # and generate augmented data for each row in the chunk on the fly\n",
    "        for index, row in data_frame.loc[start:end].iterrows():\n",
    "            X_batch[j], y_batch[j] = get_augmented_row(row)\n",
    "            j += 1\n",
    "\n",
    "        i += 1\n",
    "        if i == batches_per_epoch - 1:\n",
    "            # reset the index so that we can cycle over the data_frame again\n",
    "            i = 0\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    # model.add(Lambda(preprocess_batch, input_shape=(160, 320, 3), output_shape=(64, 64, 3)))\n",
    "\n",
    "    # layer 1 output shape is 32x32x32\n",
    "    model.add(Convolution2D(32, 5, 5, input_shape=(64, 64, 3), subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # layer 2 output shape is 15x15x16\n",
    "    model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=\"valid\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(MaxPooling2D((2, 2), border_mode='valid'))\n",
    "\n",
    "    # layer 3 output shape is 12x12x16\n",
    "    model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=\"valid\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(.4))\n",
    "\n",
    "    # Flatten the output\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # layer 4\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # layer 5\n",
    "    model.add(Dense(512))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # Finally a single output, since this is a regression problem\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    data_frame = pd.read_csv('udacity_data/driving_log.csv', usecols=[0, 1, 2, 3])\n",
    "\n",
    "    # shuffle the data\n",
    "    data_frame = data_frame.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # 80-20 training validation split\n",
    "    training_split = 0.8\n",
    "\n",
    "    num_rows_training = int(data_frame.shape[0]*training_split)\n",
    "\n",
    "    training_data = data_frame.loc[0:num_rows_training-1]\n",
    "    validation_data = data_frame.loc[num_rows_training:]\n",
    "\n",
    "    # release the main data_frame from memory\n",
    "    data_frame = None\n",
    "\n",
    "    training_generator = get_data_generator(training_data, batch_size=BATCH_SIZE)\n",
    "    validation_data_generator = get_data_generator(validation_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    samples_per_epoch = (20000//BATCH_SIZE)*BATCH_SIZE\n",
    "\n",
    "    model.fit_generator(training_generator, validation_data=validation_data_generator,\n",
    "                        samples_per_epoch=samples_per_epoch, nb_epoch=3, nb_val_samples=3000)\n",
    "\n",
    "    print(\"Saving model weights and configuration file.\")\n",
    "\n",
    "    model.save_weights('model.h5')  # always save your weights after training or during training\n",
    "    with open('model.json', 'w') as outfile:\n",
    "        outfile.write(model.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
